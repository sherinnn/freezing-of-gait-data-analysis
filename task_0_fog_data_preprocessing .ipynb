{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d30b02a",
   "metadata": {},
   "source": [
    "## Things achieved through preprocessing:\n",
    "- removed 0 annotation\n",
    "- defined windows of length 4s (250 samples per window - calculated using sampling interval) and 0.5s step size for each run\n",
    "- performed feature engineering by creating window level metrics such as RMS, variance, Band power 0.5–3 Hz, Band power 3–8 Hz, Freeze Index\n",
    "- created labels - FoG v/s non FoG\n",
    "- merged into one csv for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159366f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4f4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining input and output folders\n",
    "\n",
    "INPUT_FOLDER = \"./daphnet+freezing+of+gait/dataset_fog_release/dataset\"     \n",
    "OUTPUT_FOLDER = \"./modified_dataset\" \n",
    "FILE_PATTERN = re.compile(r\"S(\\d+)R(\\d+)\\.txt$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ca577",
   "metadata": {},
   "source": [
    "## Including subject_id and run_id as columns in each individual txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac2ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed S01R01.txt → ./modified_dataset\\S01R01.txt\n",
      "Processed S01R02.txt → ./modified_dataset\\S01R02.txt\n",
      "Processed S02R01.txt → ./modified_dataset\\S02R01.txt\n",
      "Processed S02R02.txt → ./modified_dataset\\S02R02.txt\n",
      "Processed S03R01.txt → ./modified_dataset\\S03R01.txt\n",
      "Processed S03R02.txt → ./modified_dataset\\S03R02.txt\n",
      "Processed S03R03.txt → ./modified_dataset\\S03R03.txt\n",
      "Processed S04R01.txt → ./modified_dataset\\S04R01.txt\n",
      "Processed S05R01.txt → ./modified_dataset\\S05R01.txt\n",
      "Processed S05R02.txt → ./modified_dataset\\S05R02.txt\n",
      "Processed S06R01.txt → ./modified_dataset\\S06R01.txt\n",
      "Processed S06R02.txt → ./modified_dataset\\S06R02.txt\n",
      "Processed S07R01.txt → ./modified_dataset\\S07R01.txt\n",
      "Processed S07R02.txt → ./modified_dataset\\S07R02.txt\n",
      "Processed S08R01.txt → ./modified_dataset\\S08R01.txt\n",
      "Processed S09R01.txt → ./modified_dataset\\S09R01.txt\n",
      "Processed S10R01.txt → ./modified_dataset\\S10R01.txt\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "# create output folder if it doesnt exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(INPUT_FOLDER):\n",
    "    match = FILE_PATTERN.match(filename)\n",
    "    if not match:\n",
    "        continue  # skip files that dont match pattern\n",
    "\n",
    "    ss = int(match.group(1))\n",
    "    rr = int(match.group(2))\n",
    "\n",
    "    input_path = os.path.join(INPUT_FOLDER, filename)\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "\n",
    "    data = np.loadtxt(input_path)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "\n",
    "    ss_col = np.full((data.shape[0], 1), ss)\n",
    "    rr_col = np.full((data.shape[0], 1), rr)\n",
    "\n",
    "    new_data = np.hstack((data, ss_col, rr_col))\n",
    "\n",
    "    np.savetxt(output_path, new_data, fmt=\"%d\")\n",
    "\n",
    "    print(f\"Processed {filename} → {output_path}\")\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39e7ae",
   "metadata": {},
   "source": [
    "## Removing rows with Annotation 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2676c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered S01R01.txt: 151987 → 92802 rows\n",
      "Filtered S01R02.txt: 52095 → 28801 rows\n",
      "Filtered S02R01.txt: 72561 → 25601 rows\n",
      "Filtered S02R02.txt: 89645 → 64961 rows\n",
      "Filtered S03R01.txt: 144190 → 90882 rows\n",
      "Filtered S03R02.txt: 38774 → 16641 rows\n",
      "Filtered S03R03.txt: 70269 → 21121 rows\n",
      "Filtered S04R01.txt: 195737 → 132482 rows\n",
      "Filtered S05R01.txt: 109886 → 67844 rows\n",
      "Filtered S05R02.txt: 100746 → 65922 rows\n",
      "Filtered S06R01.txt: 175707 → 107523 rows\n",
      "Filtered S06R02.txt: 44227 → 19842 rows\n",
      "Filtered S07R01.txt: 119525 → 74241 rows\n",
      "Filtered S07R02.txt: 50335 → 28801 rows\n",
      "Filtered S08R01.txt: 136589 → 49284 rows\n",
      "Filtered S09R01.txt: 172311 → 111365 rows\n",
      "Filtered S10R01.txt: 193303 → 142722 rows\n",
      "All files updated.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "\n",
    "    data = np.loadtxt(file_path)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "\n",
    "    # keep rows where 11th column != 0\n",
    "    filtered_data = data[data[:, 10] != 0]\n",
    "\n",
    "    if filtered_data.size == 0:\n",
    "        open(file_path, \"w\").close()\n",
    "    else:\n",
    "        np.savetxt(file_path, filtered_data.astype(int), fmt=\"%d\")\n",
    "\n",
    "    print(f\"Filtered {filename}: {data.shape[0]} → {filtered_data.shape[0]} rows\")\n",
    "\n",
    "print(\"All files updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec407c",
   "metadata": {},
   "source": [
    "## Dividing each file into windows and creating labels - FoG and non FoG\n",
    "\n",
    "A window label answers this question:\n",
    "“During this 4-second window, was the person mostly freezing or not?”\n",
    "\n",
    "So the goal is to create labels like so:\n",
    "- window label = 1 (FOG)\n",
    "- window label = 0 (no FOG)\n",
    "\n",
    "Steps:\n",
    "1. Start with window length 4s and windowing 0.5s - following as per paper\n",
    "2. In each window, if more than 50% samples have annotation 2, then keep window label as 1 (FoG) else 0 (non FoG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab180776",
   "metadata": {},
   "source": [
    "## Checking random files to check the sampling interval and sampling frequency \n",
    "sampling interval should be 16 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f633b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling interval dt = 0.016000 s\n",
      "Sampling frequency fs = 62.50 Hz\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\".\\modified_dataset\\S05R01.txt\")\n",
    "\n",
    "t_ms = data[:, 0]  \n",
    "\n",
    "dt = np.median(np.diff(t_ms)) / 1000\n",
    "fs = 1.0 / dt\n",
    "\n",
    "print(f\"Sampling interval dt = {dt:.6f} s\")\n",
    "print(f\"Sampling frequency fs = {fs:.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606899d",
   "metadata": {},
   "source": [
    "##  Using this, I will determine the rows in each window\n",
    "\n",
    "62.5 samples are recorded every second\n",
    "\n",
    "Window length: 4 seconds × 62.5 Hz = 250 samples\n",
    "i.e. A 4-second window contains 250 consecutive samples\n",
    "\n",
    "Window Step: 0.5 seconds × 62.5 Hz ≈ 31 samples i.e.\n",
    "After making one window, we move forward 31 rows and make the next window. \n",
    "\n",
    "(4s and 0.5s determined from the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67362cef",
   "metadata": {},
   "source": [
    "Eg:\n",
    "    \n",
    "- Window 0: samples 0 to 249\n",
    "- Window 1: samples 31 to 280\n",
    "- Window 2: samples 62 to 311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540d0e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39b1780",
   "metadata": {},
   "source": [
    "### create the window\n",
    "### for each window:  \n",
    "\n",
    "-  using 250 ankle_vert values, compute RMS, variance, Band power 0.5–3 Hz, Band power 3–8 Hz, Freeze Index\n",
    "-  create a new column, if more than 50% of samples have label 2, then label will be 1, else 0, \n",
    "- Then I will generate a CSV for each txt having window level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e3d996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01R01.txt → S01R01_windows.csv (2994 windows)\n",
      "S01R02.txt → S01R02_windows.csv (930 windows)\n",
      "S02R01.txt → S02R01_windows.csv (826 windows)\n",
      "S02R02.txt → S02R02_windows.csv (2096 windows)\n",
      "S03R01.txt → S03R01_windows.csv (2932 windows)\n",
      "S03R02.txt → S03R02_windows.csv (537 windows)\n",
      "S03R03.txt → S03R03_windows.csv (682 windows)\n",
      "S04R01.txt → S04R01_windows.csv (4274 windows)\n",
      "S05R01.txt → S05R01_windows.csv (2189 windows)\n",
      "S05R02.txt → S05R02_windows.csv (2127 windows)\n",
      "S06R01.txt → S06R01_windows.csv (3469 windows)\n",
      "S06R02.txt → S06R02_windows.csv (641 windows)\n",
      "S07R01.txt → S07R01_windows.csv (2395 windows)\n",
      "S07R02.txt → S07R02_windows.csv (930 windows)\n",
      "S08R01.txt → S08R01_windows.csv (1590 windows)\n",
      "S09R01.txt → S09R01_windows.csv (3593 windows)\n",
      "S10R01.txt → S10R01_windows.csv (4604 windows)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ANKLE_VERT_COL = 2  \n",
    "ANNOT_COL = 10     \n",
    "\n",
    "WINDOW_LEN = 250\n",
    "STEP = 31\n",
    "FS = 62.5  # Hz\n",
    "\n",
    "\n",
    "\n",
    "def bandpower(x, fs, fmin, fmax):\n",
    "    if len(x) < 4:\n",
    "        return 0.0\n",
    "\n",
    "    nperseg = min(256, len(x))\n",
    "    freqs, psd = welch(x, fs=fs, nperseg=nperseg, detrend=\"constant\")\n",
    "\n",
    "    mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.trapz(psd[mask], freqs[mask]))\n",
    "\n",
    "\n",
    "def compute_features(x):\n",
    "    x = x.astype(float)\n",
    "\n",
    "    rms = float(np.sqrt(np.mean(x * x))) if len(x) else 0.0\n",
    "    var = float(np.var(x)) if len(x) else 0.0\n",
    "\n",
    "    bp_lo = bandpower(x, FS, 0.5, 3.0)\n",
    "    bp_hi = bandpower(x, FS, 3.0, 8.0)\n",
    "\n",
    "    freeze_index = bp_hi / (bp_lo + 1e-12)\n",
    "\n",
    "    return rms, var, bp_lo, bp_hi, freeze_index\n",
    "\n",
    "\n",
    "def iter_windows(n, win_len, step):\n",
    "    w = 0\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + win_len, n)\n",
    "        yield w, start, end\n",
    "        w += 1\n",
    "        start += step\n",
    "\n",
    "\n",
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    if not filename.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "    data = np.loadtxt(path)\n",
    "\n",
    "    if data.size == 0:\n",
    "        continue\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "\n",
    "    ankle = data[:, ANKLE_VERT_COL]\n",
    "    annot = data[:, ANNOT_COL].astype(int)\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for window_id, start, end in iter_windows(len(data), WINDOW_LEN, STEP):\n",
    "        sig = ankle[start:end]\n",
    "        ann = annot[start:end]\n",
    "\n",
    "        rms, var, bp_lo, bp_hi, fi = compute_features(sig)\n",
    "\n",
    "        freeze_ratio = float(np.mean(ann == 2)) if len(ann) else 0.0\n",
    "        label = int(freeze_ratio >= 0.5)\n",
    "\n",
    "        out_rows.append([\n",
    "            window_id,\n",
    "            start,\n",
    "            end,\n",
    "            end - start,\n",
    "            rms,\n",
    "            var,\n",
    "            bp_lo,\n",
    "            bp_hi,\n",
    "            fi,\n",
    "            freeze_ratio,\n",
    "            label\n",
    "        ])\n",
    "\n",
    "    out_csv = os.path.join(\n",
    "        OUTPUT_FOLDER,\n",
    "        filename.replace(\".txt\", \"_windows.csv\")\n",
    "    )\n",
    "\n",
    "    header = (\n",
    "        \"window_id,start_idx,end_idx_excl,n_samples,\"\n",
    "        \"rms,var,bp_0p5_3,bp_3_8,freeze_index,freeze_ratio,label\"\n",
    "    )\n",
    "\n",
    "    with open(out_csv, \"w\") as f:\n",
    "        f.write(header + \"\\n\")\n",
    "        for r in out_rows:\n",
    "            f.write(\",\".join(map(str, r)) + \"\\n\")\n",
    "\n",
    "    print(f\"{filename} → {os.path.basename(out_csv)} ({len(out_rows)} windows)\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52866bb9",
   "metadata": {},
   "source": [
    "## including subject_id and run_id in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a6e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated S01R01_windows.csv: subject_id=1, run_id=1\n",
      "Updated S01R02_windows.csv: subject_id=1, run_id=2\n",
      "Updated S02R01_windows.csv: subject_id=2, run_id=1\n",
      "Updated S02R02_windows.csv: subject_id=2, run_id=2\n",
      "Updated S03R01_windows.csv: subject_id=3, run_id=1\n",
      "Updated S03R02_windows.csv: subject_id=3, run_id=2\n",
      "Updated S03R03_windows.csv: subject_id=3, run_id=3\n",
      "Updated S04R01_windows.csv: subject_id=4, run_id=1\n",
      "Updated S05R01_windows.csv: subject_id=5, run_id=1\n",
      "Updated S05R02_windows.csv: subject_id=5, run_id=2\n",
      "Updated S06R01_windows.csv: subject_id=6, run_id=1\n",
      "Updated S06R02_windows.csv: subject_id=6, run_id=2\n",
      "Updated S07R01_windows.csv: subject_id=7, run_id=1\n",
      "Updated S07R02_windows.csv: subject_id=7, run_id=2\n",
      "Updated S08R01_windows.csv: subject_id=8, run_id=1\n",
      "Updated S09R01_windows.csv: subject_id=9, run_id=1\n",
      "Updated S10R01_windows.csv: subject_id=10, run_id=1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSV_SUFFIX = \"_windows.csv\"\n",
    "\n",
    "\n",
    "# Regex to extract ss and rr\n",
    "NAME_RE = re.compile(r\"S(\\d+)R(\\d+)_windows\\.csv$\", re.IGNORECASE)\n",
    "\n",
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    if not filename.lower().endswith(CSV_SUFFIX):\n",
    "        continue\n",
    "\n",
    "    match = NAME_RE.match(filename)\n",
    "    if not match:\n",
    "        print(f\"Skipped (name does not match pattern): {filename}\")\n",
    "        continue\n",
    "\n",
    "    ss = int(match.group(1))\n",
    "    rr = int(match.group(2))\n",
    "\n",
    "    path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not lines:\n",
    "        print(f\"Skipped empty file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    header = lines[0].strip()\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    new_header = header + \",subject_id,run_id\"\n",
    "\n",
    "    new_lines = [new_header + \"\\n\"]\n",
    "    for line in data_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        new_lines.append(f\"{line},{ss},{rr}\\n\")\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        f.writelines(new_lines)\n",
    "\n",
    "    print(f\"Updated {filename}: subject_id={ss}, run_id={rr}\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57dabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9407e5c",
   "metadata": {},
   "source": [
    "## Merging all files into one unified CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8087683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 17 files into merged.csv\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./csvs\"\n",
    "output_file = \"merged.csv\"\n",
    "\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged {len(all_files)} files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27750e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ea14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
